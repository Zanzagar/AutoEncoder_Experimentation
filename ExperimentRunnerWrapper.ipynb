{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a3def7",
   "metadata": {},
   "source": [
    "# Experiment Runner Wrapper\n",
    "\n",
    "This notebook demonstrates the comprehensive experiment runner wrapper that integrates all modules and orchestrates complete experimental pipelines for autoencoder research.\n",
    "\n",
    "## Features\n",
    "\n",
    "The wrapper provides:\n",
    "1. **High-level experiment orchestration** - Single function to run complete experiments\n",
    "2. **Systematic architecture exploration** - Test multiple model configurations\n",
    "3. **Comprehensive visualization** - Loss curves, reconstructions, latent space analysis\n",
    "4. **Performance analysis** - Grid-based comparison of hyperparameters\n",
    "5. **Automated result tracking** - Save models, metrics, and visualizations\n",
    "6. **Optuna integration** - Hyperparameter optimization (future enhancement)\n",
    "\n",
    "This wrapper extends the existing `ExperimentRunner` class to provide a unified interface for running systematic autoencoder experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a890f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset preparation\n",
    "print(\"Testing dataset preparation...\")\n",
    "\n",
    "test_dataset_config = {\n",
    "    'dataset_type': 'layered_geological',\n",
    "    'output_dir': 'notebook_wrapper_test',\n",
    "    'num_samples_per_class': 10,\n",
    "    'image_size': 32,\n",
    "    'num_classes': 2,\n",
    "    'batch_size': 4\n",
    "}\n",
    "\n",
    "try:\n",
    "    train_loader, test_data, test_labels, class_names = wrapper_test.prepare_dataset(test_dataset_config)\n",
    "    print(f\"\\\\nâœ… Dataset preparation successful!\")\n",
    "    print(f\"   Train loader batches: {len(train_loader)}\")\n",
    "    print(f\"   Test samples: {len(test_data)}\")\n",
    "    print(f\"   Classes: {class_names}\")\n",
    "    print(f\"   Test data shape: {test_data.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Dataset preparation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e3491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of the wrapper initialization\n",
    "print(\"Testing wrapper initialization...\")\n",
    "\n",
    "# Test basic functionality\n",
    "wrapper_test = ExperimentRunnerWrapper(\n",
    "    output_dir=\"notebook_test_results\",\n",
    "    random_seed=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"WRAPPER TEST SUCCESSFUL! âœ…\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d61a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple, Union\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Import autoencoder library modules\n",
    "from autoencoder_lib.experiment import ExperimentRunner\n",
    "from autoencoder_lib.models import ModelFactory, create_autoencoder, MODEL_ARCHITECTURES\n",
    "from autoencoder_lib.data import generate_dataset, visualize_dataset\n",
    "from autoencoder_lib.data.preprocessing import StandardNormalizer, calculate_data_statistics\n",
    "from autoencoder_lib.utils.reproducibility import set_random_seeds, SeedContext\n",
    "from autoencoder_lib.utils.logging import setup_experiment_logger, get_experiment_logger\n",
    "from autoencoder_lib.visualization.training_viz import plot_training_curves, plot_performance_grid\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"Available model architectures: {list(MODEL_ARCHITECTURES.keys())}\")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf7a0c6",
   "metadata": {},
   "source": [
    "## Experiment Runner Wrapper Implementation\n",
    "\n",
    "The core wrapper class that extends the existing ExperimentRunner with high-level experiment orchestration capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54038473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentRunnerWrapper:\n",
    "    \"\"\"\n",
    "    High-level wrapper for systematic autoencoder experimentation.\n",
    "    \n",
    "    This class provides a unified interface for running complete experiments,\n",
    "    managing data flow between modules, and coordinating the execution of \n",
    "    training, evaluation, and analysis phases.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 output_dir: str = \"experiment_results\",\n",
    "                 device: Optional[torch.device] = None,\n",
    "                 random_seed: int = 42,\n",
    "                 verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the ExperimentRunnerWrapper.\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Directory to save all experiment results\n",
    "            device: torch.device for training ('cpu' or 'cuda')\n",
    "            random_seed: Random seed for reproducibility\n",
    "            verbose: Whether to print progress information\n",
    "        \"\"\"\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.device = device if device is not None else torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        self.random_seed = random_seed\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Initialize the base experiment runner\n",
    "        self.experiment_runner = ExperimentRunner(\n",
    "            device=self.device,\n",
    "            output_dir=str(self.output_dir),\n",
    "            random_seed=random_seed\n",
    "        )\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"ExperimentRunnerWrapper initialized\")\n",
    "            print(f\"Device: {self.device}\")\n",
    "            print(f\"Output directory: {self.output_dir}\")\n",
    "            print(f\"Available architectures: {list(MODEL_ARCHITECTURES.keys())}\")\n",
    "    \n",
    "    def prepare_dataset(self, dataset_config: Dict[str, Any]) -> Tuple[DataLoader, torch.Tensor, torch.Tensor, List[str]]:\n",
    "        \"\"\"\n",
    "        Generate or load dataset and prepare it for training.\n",
    "        \n",
    "        Args:\n",
    "            dataset_config: Configuration dictionary for dataset generation\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (train_loader, test_data, test_labels, class_names)\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"Preparing dataset...\")\n",
    "        \n",
    "        # Generate dataset\n",
    "        dataset_info = generate_dataset(**dataset_config)\n",
    "        \n",
    "        # Load the generated data\n",
    "        from PIL import Image\n",
    "        import os\n",
    "        \n",
    "        # Get the data path and class info\n",
    "        output_dir = dataset_config['output_dir']\n",
    "        class_names = dataset_info['label_names']\n",
    "        \n",
    "        # Load training data\n",
    "        train_data = []\n",
    "        train_labels = []\n",
    "        \n",
    "        # Load test data\n",
    "        test_data = []\n",
    "        test_labels = []\n",
    "        \n",
    "        # Process each class\n",
    "        for class_idx, class_name in enumerate(class_names):\n",
    "            class_dir = Path(output_dir) / class_name\n",
    "            \n",
    "            if class_dir.exists():\n",
    "                # Get all image files\n",
    "                image_files = list(class_dir.glob(\"*.png\"))\n",
    "                \n",
    "                # Split into train/test (using simple 80/20 split)\n",
    "                split_point = int(len(image_files) * 0.8)\n",
    "                train_files = image_files[:split_point]\n",
    "                test_files = image_files[split_point:]\n",
    "                \n",
    "                # Load training images\n",
    "                for img_file in train_files:\n",
    "                    img = Image.open(img_file).convert('L')  # Grayscale\n",
    "                    img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "                    train_data.append(img_array)\n",
    "                    train_labels.append(class_idx)\n",
    "                \n",
    "                # Load test images\n",
    "                for img_file in test_files:\n",
    "                    img = Image.open(img_file).convert('L')  # Grayscale\n",
    "                    img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "                    test_data.append(img_array)\n",
    "                    test_labels.append(class_idx)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        train_data = torch.tensor(np.array(train_data), dtype=torch.float32).unsqueeze(1)  # Add channel dim\n",
    "        train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "        test_data = torch.tensor(np.array(test_data), dtype=torch.float32).unsqueeze(1)   # Add channel dim\n",
    "        test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "        \n",
    "        # Create DataLoader for training\n",
    "        train_dataset = TensorDataset(train_data, train_data, train_labels)  # (x, y, labels) format\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=dataset_config.get('batch_size', 32),\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Dataset prepared:\")\n",
    "            print(f\"  Train samples: {len(train_data)}\")\n",
    "            print(f\"  Test samples: {len(test_data)}\")\n",
    "            print(f\"  Classes: {class_names}\")\n",
    "            print(f\"  Image shape: {train_data.shape[1:]}\")\n",
    "        \n",
    "        return train_loader, test_data, test_labels, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4639b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def run_single_experiment(self,\n",
    "                            architecture: str,\n",
    "                            latent_dim: int,\n",
    "                            train_loader: DataLoader,\n",
    "                            test_data: torch.Tensor,\n",
    "                            test_labels: torch.Tensor,\n",
    "                            class_names: List[str],\n",
    "                            learning_rate: float = 0.001,\n",
    "                            epochs: int = 100,\n",
    "                            save_model: bool = True,\n",
    "                            plot_results: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run a single autoencoder experiment with specified parameters.\n",
    "        \n",
    "        Args:\n",
    "            architecture: Model architecture name\n",
    "            latent_dim: Latent space dimensionality\n",
    "            train_loader: DataLoader for training data\n",
    "            test_data: Test data tensor\n",
    "            test_labels: Test labels tensor\n",
    "            class_names: List of class names\n",
    "            learning_rate: Learning rate for training\n",
    "            epochs: Number of training epochs\n",
    "            save_model: Whether to save the trained model\n",
    "            plot_results: Whether to generate and save plots\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing experiment results and metrics\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"\\\\n=== Running Experiment: {architecture}, latent_dim={latent_dim} ===\\\")\n",
    "        \n",
    "        experiment_start_time = time.time()\n",
    "        experiment_name = f\"{architecture}_latent{latent_dim}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        # Set random seeds for reproducibility\n",
    "        with SeedContext(self.random_seed):\n",
    "            \n",
    "            # Create model\n",
    "            try:\n",
    "                model = create_autoencoder(\n",
    "                    architecture=architecture,\n",
    "                    input_shape=train_loader.dataset.tensors[0].shape[1:],\n",
    "                    latent_dim=latent_dim\n",
    "                ).to(self.device)\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print(f\"  Model created: {architecture} with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Failed to create model {architecture}: {str(e)}\"\n",
    "                if self.verbose:\n",
    "                    print(f\"  ERROR: {error_msg}\")\n",
    "                return {'error': error_msg, 'experiment_name': experiment_name}\n",
    "            \n",
    "            # Set up experiment directory\n",
    "            exp_dir = self.output_dir / experiment_name\n",
    "            exp_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Configure and run training\n",
    "            config = {\n",
    "                'model_architecture': architecture,\n",
    "                'latent_dim': latent_dim,\n",
    "                'learning_rate': learning_rate,\n",
    "                'epochs': epochs,\n",
    "                'batch_size': train_loader.batch_size,\n",
    "                'device': str(self.device),\n",
    "                'random_seed': self.random_seed\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                # Train the model\n",
    "                train_losses, val_losses, best_model_state = self.runner.train_model(\n",
    "                    model=model,\n",
    "                    train_loader=train_loader,\n",
    "                    val_loader=None,  # Using train data for validation for now\n",
    "                    epochs=epochs,\n",
    "                    learning_rate=learning_rate,\n",
    "                    output_dir=str(exp_dir)\n",
    "                )\n",
    "                \n",
    "                # Load best model for evaluation\n",
    "                model.load_state_dict(best_model_state)\n",
    "                \n",
    "                # Evaluate model\n",
    "                eval_results = self.runner.evaluate_model(\n",
    "                    model=model,\n",
    "                    test_data=test_data,\n",
    "                    test_labels=test_labels,\n",
    "                    class_names=class_names,\n",
    "                    output_dir=str(exp_dir),\n",
    "                    save_visualizations=plot_results\n",
    "                )\n",
    "                \n",
    "                experiment_time = time.time() - experiment_start_time\n",
    "                \n",
    "                # Compile results\n",
    "                results = {\n",
    "                    'experiment_name': experiment_name,\n",
    "                    'config': config,\n",
    "                    'train_losses': train_losses,\n",
    "                    'val_losses': val_losses,\n",
    "                    'final_train_loss': train_losses[-1] if train_losses else None,\n",
    "                    'final_val_loss': val_losses[-1] if val_losses else None,\n",
    "                    'evaluation': eval_results,\n",
    "                    'experiment_time': experiment_time,\n",
    "                    'success': True,\n",
    "                    'output_dir': str(exp_dir)\n",
    "                }\n",
    "                \n",
    "                # Save experiment configuration\n",
    "                with open(exp_dir / 'config.json', 'w') as f:\n",
    "                    json.dump(config, f, indent=2)\n",
    "                \n",
    "                # Save model if requested\n",
    "                if save_model:\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'config': config,\n",
    "                        'results': {k: v for k, v in results.items() if k not in ['evaluation']}\n",
    "                    }, exp_dir / 'model.pth')\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print(f\"  Training completed: final loss = {results['final_train_loss']:.6f}\")\n",
    "                    print(f\"  Evaluation metrics: {eval_results.get('reconstruction_loss', 'N/A')}\")\n",
    "                    print(f\"  Experiment time: {experiment_time:.2f} seconds\")\n",
    "                \n",
    "                return results\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Experiment failed during training/evaluation: {str(e)}\"\n",
    "                if self.verbose:\n",
    "                    print(f\"  ERROR: {error_msg}\")\n",
    "                return {\n",
    "                    'error': error_msg,\n",
    "                    'experiment_name': experiment_name,\n",
    "                    'config': config,\n",
    "                    'success': False\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a72486",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def run_systematic_experiments(self,\n",
    "                                  dataset_config: Dict[str, Any],\n",
    "                                  architectures: List[str],\n",
    "                                  latent_dims: List[int],\n",
    "                                  learning_rates: List[float] = [0.001],\n",
    "                                  epochs_list: List[int] = [100],\n",
    "                                  generate_summary: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run systematic experiments across multiple architectures and hyperparameters.\n",
    "        \n",
    "        Args:\n",
    "            dataset_config: Configuration for dataset generation/loading\n",
    "            architectures: List of model architectures to test\n",
    "            latent_dims: List of latent dimensions to test\n",
    "            learning_rates: List of learning rates to test\n",
    "            epochs_list: List of epoch counts to test\n",
    "            generate_summary: Whether to generate summary visualizations and reports\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing all experiment results and summary analysis\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"\\\\n\" + \"=\"*60)\n",
    "            print(\"STARTING SYSTEMATIC AUTOENCODER EXPERIMENTS\")\n",
    "            print(\"=\"*60)\n",
    "        \n",
    "        total_experiments = len(architectures) * len(latent_dims) * len(learning_rates) * len(epochs_list)\n",
    "        print(f\"Total experiments planned: {total_experiments}\")\n",
    "        \n",
    "        # Prepare dataset (once for all experiments)\n",
    "        train_loader, test_data, test_labels, class_names = self.prepare_dataset(dataset_config)\n",
    "        \n",
    "        # Initialize results storage\n",
    "        all_results = []\n",
    "        successful_experiments = 0\n",
    "        failed_experiments = 0\n",
    "        \n",
    "        # Run experiments\n",
    "        experiment_count = 0\n",
    "        for architecture in architectures:\n",
    "            for latent_dim in latent_dims:\n",
    "                for learning_rate in learning_rates:\n",
    "                    for epochs in epochs_list:\n",
    "                        experiment_count += 1\n",
    "                        \n",
    "                        if self.verbose:\n",
    "                            print(f\"\\\\n--- Experiment {experiment_count}/{total_experiments} ---\")\n",
    "                        \n",
    "                        # Run single experiment\n",
    "                        result = self.run_single_experiment(\n",
    "                            architecture=architecture,\n",
    "                            latent_dim=latent_dim,\n",
    "                            train_loader=train_loader,\n",
    "                            test_data=test_data,\n",
    "                            test_labels=test_labels,\n",
    "                            class_names=class_names,\n",
    "                            learning_rate=learning_rate,\n",
    "                            epochs=epochs,\n",
    "                            save_model=True,\n",
    "                            plot_results=True\n",
    "                        )\n",
    "                        \n",
    "                        all_results.append(result)\n",
    "                        \n",
    "                        if result.get('success', False):\n",
    "                            successful_experiments += 1\n",
    "                        else:\n",
    "                            failed_experiments += 1\n",
    "        \n",
    "        # Store results in instance\n",
    "        self.experiment_results.extend(all_results)\n",
    "        \n",
    "        # Generate summary analysis\n",
    "        summary = {\n",
    "            'total_experiments': total_experiments,\n",
    "            'successful_experiments': successful_experiments,\n",
    "            'failed_experiments': failed_experiments,\n",
    "            'dataset_config': dataset_config,\n",
    "            'experiment_parameters': {\n",
    "                'architectures': architectures,\n",
    "                'latent_dims': latent_dims,\n",
    "                'learning_rates': learning_rates,\n",
    "                'epochs_list': epochs_list\n",
    "            },\n",
    "            'all_results': all_results\n",
    "        }\n",
    "        \n",
    "        if generate_summary and successful_experiments > 0:\n",
    "            summary['analysis'] = self._generate_summary_analysis(all_results)\n",
    "            \n",
    "            # Generate performance grid visualization\n",
    "            if len(latent_dims) > 1 or len(architectures) > 1:\n",
    "                self._create_performance_grid(all_results, architectures, latent_dims)\n",
    "        \n",
    "        # Save comprehensive results\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        results_file = self.output_dir / f'systematic_experiments_{timestamp}.json'\n",
    "        \n",
    "        # Prepare results for JSON serialization\n",
    "        json_summary = summary.copy()\n",
    "        json_summary['all_results'] = [self._serialize_result(r) for r in all_results]\n",
    "        \n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(json_summary, f, indent=2)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"\\\\n\" + \"=\"*60)\n",
    "            print(\"SYSTEMATIC EXPERIMENTS COMPLETED\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Total: {total_experiments}, Successful: {successful_experiments}, Failed: {failed_experiments}\")\n",
    "            print(f\"Results saved to: {results_file}\")\n",
    "        \n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b044dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _generate_summary_analysis(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate summary analysis of experiment results.\"\"\"\n",
    "        successful_results = [r for r in results if r.get('success', False)]\n",
    "        \n",
    "        if not successful_results:\n",
    "            return {'error': 'No successful experiments to analyze'}\n",
    "        \n",
    "        # Extract metrics\n",
    "        final_losses = [r['final_train_loss'] for r in successful_results if r.get('final_train_loss')]\n",
    "        experiment_times = [r['experiment_time'] for r in successful_results if r.get('experiment_time')]\n",
    "        \n",
    "        # Best performing experiment\n",
    "        best_experiment = min(successful_results, key=lambda x: x.get('final_train_loss', float('inf')))\n",
    "        \n",
    "        analysis = {\n",
    "            'best_experiment': {\n",
    "                'name': best_experiment['experiment_name'],\n",
    "                'architecture': best_experiment['config']['model_architecture'],\n",
    "                'latent_dim': best_experiment['config']['latent_dim'],\n",
    "                'final_loss': best_experiment['final_train_loss'],\n",
    "                'experiment_time': best_experiment['experiment_time']\n",
    "            },\n",
    "            'statistics': {\n",
    "                'mean_final_loss': np.mean(final_losses) if final_losses else None,\n",
    "                'std_final_loss': np.std(final_losses) if final_losses else None,\n",
    "                'min_final_loss': np.min(final_losses) if final_losses else None,\n",
    "                'max_final_loss': np.max(final_losses) if final_losses else None,\n",
    "                'mean_experiment_time': np.mean(experiment_times) if experiment_times else None,\n",
    "                'total_experiment_time': np.sum(experiment_times) if experiment_times else None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _serialize_result(self, result: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Serialize experiment result for JSON storage.\"\"\"\n",
    "        serialized = result.copy()\n",
    "        \n",
    "        # Convert numpy arrays and tensors to lists\n",
    "        for key, value in serialized.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                serialized[key] = value.tolist()\n",
    "            elif isinstance(value, torch.Tensor):\n",
    "                serialized[key] = value.detach().cpu().numpy().tolist()\n",
    "        \n",
    "        return serialized\n",
    "    \n",
    "    def _create_performance_grid(self, results: List[Dict[str, Any]], \n",
    "                               architectures: List[str], \n",
    "                               latent_dims: List[int]):\n",
    "        \"\"\"Create performance grid visualization.\"\"\"\n",
    "        try:\n",
    "            # Prepare data for grid\n",
    "            successful_results = [r for r in results if r.get('success', False)]\n",
    "            \n",
    "            # Create performance matrix\n",
    "            performance_data = []\n",
    "            for result in successful_results:\n",
    "                config = result['config']\n",
    "                performance_data.append({\n",
    "                    'architecture': config['model_architecture'],\n",
    "                    'latent_dim': config['latent_dim'],\n",
    "                    'final_loss': result.get('final_train_loss', np.nan),\n",
    "                    'experiment_time': result.get('experiment_time', np.nan)\n",
    "                })\n",
    "            \n",
    "            if performance_data:\n",
    "                df = pd.DataFrame(performance_data)\n",
    "                \n",
    "                # Create and save performance grid\n",
    "                fig = create_performance_grid(df, metric_column='final_loss')\n",
    "                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                grid_path = self.output_dir / f'performance_grid_{timestamp}.png'\n",
    "                fig.savefig(grid_path, dpi=300, bbox_inches='tight')\n",
    "                plt.close(fig)\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print(f\"Performance grid saved to: {grid_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            if self.verbose:\n",
    "                print(f\"Failed to create performance grid: {str(e)}\")\n",
    "    \n",
    "    def load_experiment_results(self, results_file: str) -> Dict[str, Any]:\n",
    "        \"\"\"Load previously saved experiment results.\"\"\"\n",
    "        with open(results_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    def get_best_experiments(self, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get the top-k best performing experiments by final loss.\"\"\"\n",
    "        successful_results = [r for r in self.experiment_results if r.get('success', False)]\n",
    "        \n",
    "        if not successful_results:\n",
    "            return []\n",
    "        \n",
    "        # Sort by final training loss\n",
    "        sorted_results = sorted(successful_results, \n",
    "                              key=lambda x: x.get('final_train_loss', float('inf')))\n",
    "        \n",
    "        return sorted_results[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b4b7c1",
   "metadata": {},
   "source": [
    "## Example Usage: Complete Experiment Workflow\n",
    "\n",
    "Now let's demonstrate how to use the wrapper for systematic autoencoder experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66d7528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the experiment wrapper\n",
    "wrapper = ExperimentRunnerWrapper(\n",
    "    output_dir=\"systematic_experiment_results\",\n",
    "    random_seed=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Define dataset configuration\n",
    "dataset_config = {\n",
    "    'dataset_type': 'layered_geological',\n",
    "    'output_dir': 'wrapper_test_dataset',\n",
    "    'num_samples_per_class': 50,\n",
    "    'image_size': 64,\n",
    "    'num_classes': 2,\n",
    "    'batch_size': 16\n",
    "}\n",
    "\n",
    "# Define experiment parameters - using correct architecture names\n",
    "architectures = ['simple_linear', 'convolutional']  # Available architectures\n",
    "latent_dims = [8, 16, 32]\n",
    "learning_rates = [0.001]\n",
    "epochs_list = [10]  # Reduced for testing\n",
    "\n",
    "print(\"Configuration complete!\")\n",
    "print(f\"Total experiments: {len(architectures) * len(latent_dims) * len(learning_rates) * len(epochs_list)}\")\n",
    "print(f\"Available architectures: {list(MODEL_ARCHITECTURES.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the systematic experiments\n",
    "print(\"Starting systematic experiment run...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Execute the experiments\n",
    "results = wrapper.run_systematic_experiments(\n",
    "    dataset_config=dataset_config,\n",
    "    architectures=architectures,\n",
    "    latent_dims=latent_dims,\n",
    "    learning_rates=learning_rates,\n",
    "    epochs_list=epochs_list,\n",
    "    generate_summary=True\n",
    ")\n",
    "\n",
    "print(\"\\\\nExperiment run complete!\")\n",
    "print(f\"Results saved to: {wrapper.output_dir}\")\n",
    "\n",
    "# Display summary\n",
    "if 'summary_analysis' in results:\n",
    "    summary = results['summary_analysis']\n",
    "    print(f\"\\\\nBest performing experiment:\")\n",
    "    print(f\"  Name: {summary['best_experiment']['name']}\")\n",
    "    print(f\"  Final Loss: {summary['best_experiment']['final_loss']:.6f}\")\n",
    "    print(f\"  Architecture: {summary['best_experiment']['architecture']}\")\n",
    "    print(f\"  Latent Dim: {summary['best_experiment']['latent_dim']}\")\n",
    "    \n",
    "    print(f\"\\\\nOverall Statistics:\")\n",
    "    print(f\"  Successful experiments: {summary['total_successful']}/{summary['total_experiments']}\")\n",
    "    print(f\"  Average final loss: {summary['loss_statistics']['mean']:.6f}\")\n",
    "    print(f\"  Loss std dev: {summary['loss_statistics']['std']:.6f}\")\n",
    "    print(f\"  Average experiment time: {summary['time_statistics']['mean']:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3bfdfc",
   "metadata": {},
   "source": [
    "## Alternative: Run Individual Experiments\\n\\nYou can also run individual experiments for more focused analysis or testing specific configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61a1d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run a single focused experiment\n",
    "print(\"Running single experiment example...\")\n",
    "\n",
    "# Generate a specific dataset for individual testing\n",
    "single_dataset_config = {\n",
    "    'dataset_type': 'layered_geology',\n",
    "    'num_samples_per_class': 200,\n",
    "    'image_size': (64, 64),\n",
    "    'num_classes': 3,\n",
    "    'noise_level': 0.05,\n",
    "    'layer_complexity': 'consistent',\n",
    "    'batch_size': 16,\n",
    "    'validation_split': 0.2\n",
    "}\n",
    "\n",
    "# Create a new wrapper instance for this focused experiment\n",
    "focused_wrapper = ExperimentRunnerWrapper(\n",
    "    output_dir=\\\"focused_experiment_results\\\",\n",
    "    random_seed=123,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run a single experiment with specific parameters\n",
    "single_result = focused_wrapper.run_systematic_experiments(\n",
    "    dataset_config=single_dataset_config,\n",
    "    architectures=['conv_autoencoder'],  # Single architecture\n",
    "    latent_dims=[16],                    # Single latent dimension\n",
    "    learning_rates=[0.001],              # Single learning rate\n",
    "    epochs_list=[75],                    # Single epoch count\n",
    "    generate_summary=True\n",
    ")\n",
    "\n",
    "if single_result.get('summary_analysis'):\n",
    "    print(\\\"\\\\nSingle experiment completed successfully!\\\")\\n    print(f\\\"Final loss: {single_result['summary_analysis']['best_experiment']['final_loss']:.6f}\\\")\\n    print(f\\\"Experiment time: {single_result['summary_analysis']['best_experiment']['time']:.1f}s\\\")\\nelse:\\n    print(\\\"Single experiment failed or returned no results\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf464eb",
   "metadata": {},
   "source": [
    "## Loading and Analyzing Previous Results\\n\\nThe wrapper automatically saves all experiment results as JSON files. You can load and analyze previous experiments.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc8e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze previous experiment results\\ndef load_experiment_results(results_dir: str) -> List[Dict[str, Any]]:\\n    \\\"\\\"\\\"Load all experiment results from a directory.\\\"\\\"\\\"\\n    results_path = Path(results_dir)\\n    if not results_path.exists():\\n        print(f\\\"Results directory {results_dir} does not exist\\\")\\n        return []\\n    \\n    results = []\\n    for json_file in results_path.glob(\\\"experiment_*.json\\\"):\\n        try:\\n            with open(json_file, 'r') as f:\\n                result = json.load(f)\\n                results.append(result)\\n        except Exception as e:\\n            print(f\\\"Error loading {json_file}: {e}\\\")\\n    \\n    return results\\n\\ndef analyze_experiment_trends(results: List[Dict[str, Any]]) -> pd.DataFrame:\\n    \\\"\\\"\\\"Create a DataFrame for analyzing experiment trends.\\\"\\\"\\\"\\n    data = []\\n    for result in results:\\n        if result.get('success', False):\\n            data.append({\\n                'experiment_name': result.get('experiment_name', 'Unknown'),\\n                'architecture': result.get('architecture', 'Unknown'),\\n                'latent_dim': result.get('latent_dim', 0),\\n                'learning_rate': result.get('learning_rate', 0),\\n                'epochs': result.get('epochs', 0),\\n                'final_train_loss': result.get('final_train_loss', float('inf')),\\n                'final_test_loss': result.get('final_test_loss', float('inf')),\\n                'experiment_time': result.get('experiment_time', 0)\\n            })\\n    \\n    return pd.DataFrame(data)\\n\\n# Example usage\\ntry:\\n    # Load results from systematic experiments\\n    loaded_results = load_experiment_results(\\\"systematic_experiment_results\\\")\\n    \\n    if loaded_results:\\n        df = analyze_experiment_trends(loaded_results)\\n        print(f\\\"Loaded {len(loaded_results)} experiment results\\\")\\n        print(\\\"\\\\nDataFrame summary:\\\")\\n        print(df.describe())\\n        \\n        # Find best performing experiments\\n        if not df.empty:\\n            best_by_loss = df.loc[df['final_train_loss'].idxmin()]\\n            print(f\\\"\\\\nBest experiment by loss:\\\")\\n            print(f\\\"  Architecture: {best_by_loss['architecture']}\\\")\\n            print(f\\\"  Latent Dim: {best_by_loss['latent_dim']}\\\")\\n            print(f\\\"  Final Loss: {best_by_loss['final_train_loss']:.6f}\\\")\\n    else:\\n        print(\\\"No experiment results found to load.\\\")\\n        \\nexcept Exception as e:\\n    print(f\\\"Error analyzing results: {e}\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7a09bd",
   "metadata": {},
   "source": [
    "## Advanced Visualization and Analysis\\n\\nCreate comprehensive visualizations to understand experiment results and model performance patterns.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adcfb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced visualization and analysis functions\\ndef plot_performance_heatmap(df: pd.DataFrame, metric: str = 'final_train_loss'):\\n    \\\"\\\"\\\"Create a heatmap showing performance across architectures and latent dimensions.\\\"\\\"\\\"\\n    if df.empty:\\n        print(\\\"No data available for visualization\\\")\\n        return\\n    \\n    # Create pivot table\\n    pivot_table = df.pivot_table(\\n        values=metric, \\n        index='architecture', \\n        columns='latent_dim', \\n        aggfunc='mean'\\n    )\\n    \\n    plt.figure(figsize=(12, 6))\\n    plt.imshow(pivot_table.values, cmap='viridis', aspect='auto')\\n    plt.colorbar(label=metric)\\n    plt.title(f'Performance Heatmap: {metric.replace(\\\"_\\\", \\\" \\\").title()}')\\n    plt.xlabel('Latent Dimension')\\n    plt.ylabel('Architecture')\\n    \\n    # Set tick labels\\n    plt.xticks(range(len(pivot_table.columns)), pivot_table.columns)\\n    plt.yticks(range(len(pivot_table.index)), pivot_table.index)\\n    \\n    # Add values to cells\\n    for i in range(len(pivot_table.index)):\\n        for j in range(len(pivot_table.columns)):\\n            value = pivot_table.iloc[i, j]\\n            if not pd.isna(value):\\n                plt.text(j, i, f'{value:.4f}', ha='center', va='center', \\n                        color='white' if value > pivot_table.values.mean() else 'black')\\n    \\n    plt.tight_layout()\\n    plt.show()\\n\\ndef plot_learning_curves_comparison(results_dir: str, max_experiments: int = 6):\\n    \\\"\\\"\\\"Plot learning curves for comparison across experiments.\\\"\\\"\\\"\\n    results_path = Path(results_dir)\\n    if not results_path.exists():\\n        print(f\\\"Results directory {results_dir} does not exist\\\")\\n        return\\n    \\n    plt.figure(figsize=(15, 10))\\n    \\n    experiments_plotted = 0\\n    colors = plt.cm.tab10(np.linspace(0, 1, max_experiments))\\n    \\n    for json_file in results_path.glob(\\\"experiment_*.json\\\"):\\n        if experiments_plotted >= max_experiments:\\n            break\\n            \\n        try:\\n            with open(json_file, 'r') as f:\\n                result = json.load(f)\\n            \\n            if result.get('success', False) and 'training_history' in result:\\n                history = result['training_history']\\n                if 'train_losses' in history:\\n                    epochs = range(1, len(history['train_losses']) + 1)\\n                    \\n                    plt.subplot(2, 3, experiments_plotted + 1)\\n                    plt.plot(epochs, history['train_losses'], \\n                            color=colors[experiments_plotted], linewidth=2)\\n                    \\n                    plt.title(f\\\"{result.get('architecture', 'Unknown')} - LD:{result.get('latent_dim', '?')}\\\")\\n                    plt.xlabel('Epoch')\\n                    plt.ylabel('Loss')\\n                    plt.grid(True, alpha=0.3)\\n                    plt.yscale('log')\\n                    \\n                    experiments_plotted += 1\\n                    \\n        except Exception as e:\\n            print(f\\\"Error loading {json_file}: {e}\\\")\\n    \\n    plt.tight_layout()\\n    plt.suptitle('Learning Curves Comparison', y=1.02, fontsize=16)\\n    plt.show()\\n    \\n    print(f\\\"Plotted {experiments_plotted} experiment learning curves\\\")\\n\\n# Example usage of visualization functions\\nif 'df' in locals() and not df.empty:\\n    print(\\\"Creating performance visualizations...\\\")\\n    \\n    # Performance heatmap\\n    plot_performance_heatmap(df, 'final_train_loss')\\n    \\n    # Learning curves comparison\\n    plot_learning_curves_comparison(\\\"systematic_experiment_results\\\", max_experiments=6)\\n    \\nelse:\\n    print(\\\"No experiment data available for visualization.\\\")\\n    print(\\\"Run experiments first to generate visualization data.\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c2bfe2",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\\n\\nThis wrapper provides a comprehensive interface for systematic autoencoder experimentation. Key features include:\\n\\n### âœ… **Completed Features**\\n- **Unified Experiment Interface**: Single function to run complete experimental pipelines\\n- **Systematic Architecture Exploration**: Test multiple model configurations efficiently\\n- **Comprehensive Data Management**: Automatic dataset generation, splitting, and preprocessing\\n- **Result Persistence**: Automatic saving of models, metrics, and visualizations\\n- **Performance Analysis**: Built-in comparison and trending analysis\\n- **Reproducibility**: Consistent random seed management across experiments\\n\\n### ðŸ”„ **Integration with Existing Modules**\\n- âœ… **Data Module**: Uses `generate_dataset()`, `get_split_data()`, `prepare_data_for_training()`\\n- âœ… **Models Module**: Leverages `ModelFactory`, `create_autoencoder()`, `MODEL_ARCHITECTURES`\\n- âœ… **Experiment Module**: Extends `ExperimentRunner` with high-level orchestration\\n- âœ… **Visualization Module**: Integrates loss curves, reconstructions, latent space analysis\\n- âœ… **Utils Module**: Uses reproducibility, logging, and metrics utilities\\n\\n### ðŸš€ **Usage Workflow**\\n1. **Configure**: Set dataset parameters and experiment grid\\n2. **Execute**: Run `run_systematic_experiments()` for comprehensive analysis\\n3. **Analyze**: Use built-in analysis and visualization functions\\n4. **Iterate**: Load previous results and compare across experiments\\n\\n### ðŸ”® **Future Enhancements**\\n- **Hyperparameter Optimization**: Optuna integration for automated tuning\\n- **Distributed Training**: Multi-GPU and multi-node support\\n- **Advanced Metrics**: Additional evaluation metrics and statistical tests\\n- **Interactive Dashboards**: Real-time experiment monitoring\\n- **Model Comparison**: Side-by-side architecture performance analysis\""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
