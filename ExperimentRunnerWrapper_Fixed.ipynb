{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3745a0ac",
   "metadata": {},
   "source": [
    "# Experiment Runner Wrapper - WORKING VERSION\n",
    "\n",
    "This notebook demonstrates the comprehensive experiment runner wrapper that integrates all modules and orchestrates complete experimental pipelines for autoencoder research.\n",
    "\n",
    "## Test Steps:\n",
    "1. **Import libraries and test basic functionality**\n",
    "2. **Initialize the wrapper**\n",
    "3. **Test dataset preparation**\n",
    "4. **Run a simple experiment**\n",
    "5. **Analyze results**\n",
    "\n",
    "Let's start testing step by step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194dcddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import libraries and test basic functionality\n",
    "print(\"=== STEP 1: TESTING IMPORTS ===\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple, Union\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Import autoencoder library modules\n",
    "from autoencoder_lib.experiment import ExperimentRunner\n",
    "from autoencoder_lib.models import create_autoencoder, MODEL_ARCHITECTURES\n",
    "from autoencoder_lib.data import generate_dataset\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "print(f\"Available model architectures: {list(MODEL_ARCHITECTURES.keys())}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63023fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the ExperimentRunnerWrapper class\n",
    "print(\"=== STEP 2: DEFINING WRAPPER CLASS ===\")\n",
    "\n",
    "class ExperimentRunnerWrapper:\n",
    "    \\\"\\\"\\\"\n",
    "    High-level wrapper for systematic autoencoder experimentation.\n",
    "    \\\"\\\"\\\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 output_dir: str = \"experiment_results\",\n",
    "                 device: Optional[torch.device] = None,\n",
    "                 random_seed: int = 42,\n",
    "                 verbose: bool = True):\n",
    "        \\\"\\\"\\\"Initialize the ExperimentRunnerWrapper.\\\"\\\"\\\"\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.device = device if device is not None else torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        self.random_seed = random_seed\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Initialize the base experiment runner\n",
    "        self.experiment_runner = ExperimentRunner(\n",
    "            device=self.device,\n",
    "            output_dir=str(self.output_dir),\n",
    "            random_seed=random_seed\n",
    "        )\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"ExperimentRunnerWrapper initialized\")\n",
    "            print(f\"  Device: {self.device}\")\n",
    "            print(f\"  Output directory: {self.output_dir}\")\n",
    "    \n",
    "    def prepare_simple_dataset(self, dataset_config: Dict[str, Any]) -> Tuple[DataLoader, torch.Tensor, torch.Tensor, List[str]]:\n",
    "        \\\"\\\"\\\"Generate dataset and prepare it for training.\\\"\\\"\\\"\n",
    "        if self.verbose:\n",
    "            print(\"Preparing dataset...\")\n",
    "        \n",
    "        # Generate dataset\n",
    "        dataset_info = generate_dataset(**dataset_config)\n",
    "        \n",
    "        # Simple data loading (for demonstration)\n",
    "        from PIL import Image\n",
    "        import os\n",
    "        \n",
    "        output_dir = dataset_config['output_dir']\n",
    "        class_names = dataset_info['label_names']\n",
    "        \n",
    "        # Load all data\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # Process each class\n",
    "        for class_idx, class_name in enumerate(class_names):\n",
    "            class_dir = Path(output_dir) / class_name\n",
    "            \n",
    "            if class_dir.exists():\n",
    "                for img_file in class_dir.glob(\"*.png\"):\n",
    "                    img = Image.open(img_file).convert('L')  # Grayscale\n",
    "                    img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "                    all_data.append(img_array)\n",
    "                    all_labels.append(class_idx)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        all_data = torch.tensor(np.array(all_data), dtype=torch.float32).unsqueeze(1)  # Add channel dim\n",
    "        all_labels = torch.tensor(all_labels, dtype=torch.long)\n",
    "        \n",
    "        # Simple train/test split\n",
    "        split_point = int(len(all_data) * 0.8)\n",
    "        train_data = all_data[:split_point]\n",
    "        train_labels = all_labels[:split_point]\n",
    "        test_data = all_data[split_point:]\n",
    "        test_labels = all_labels[split_point:]\n",
    "        \n",
    "        # Create DataLoader for training\n",
    "        train_dataset = TensorDataset(train_data, train_data, train_labels)  # (x, y, labels) format\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=dataset_config.get('batch_size', 8),\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Dataset prepared:\")\n",
    "            print(f\"  Train samples: {len(train_data)}\")\n",
    "            print(f\"  Test samples: {len(test_data)}\")\n",
    "            print(f\"  Classes: {class_names}\")\n",
    "            print(f\"  Image shape: {train_data.shape[1:]}\")\n",
    "        \n",
    "        return train_loader, test_data, test_labels, class_names\n",
    "\n",
    "print(\"✅ ExperimentRunnerWrapper class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40cad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize the wrapper and test dataset preparation\n",
    "print(\"=== STEP 3: TESTING WRAPPER INITIALIZATION ===\")\n",
    "\n",
    "# Initialize the wrapper\n",
    "wrapper = ExperimentRunnerWrapper(\n",
    "    output_dir=\"test_wrapper_results\",\n",
    "    random_seed=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\\\n=== TESTING DATASET PREPARATION ===\")\n",
    "\n",
    "# Define a simple dataset configuration\n",
    "dataset_config = {\n",
    "    'dataset_type': 'layered_geological',\n",
    "    'output_dir': 'test_wrapper_dataset',\n",
    "    'num_samples_per_class': 8,  # Small for testing\n",
    "    'image_size': 32,\n",
    "    'num_classes': 2,\n",
    "    'batch_size': 4\n",
    "}\n",
    "\n",
    "# Test dataset preparation\n",
    "try:\n",
    "    train_loader, test_data, test_labels, class_names = wrapper.prepare_simple_dataset(dataset_config)\n",
    "    print(f\"\\\\n✅ Dataset preparation successful!\")\n",
    "    print(f\"   Train loader batches: {len(train_loader)}\")\n",
    "    print(f\"   Test samples: {len(test_data)}\")\n",
    "    print(f\"   Classes: {class_names}\")\n",
    "    print(f\"   Test data shape: {test_data.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Dataset preparation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec8c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Test model creation and simple training\n",
    "print(\"=== STEP 4: TESTING MODEL CREATION AND TRAINING ===\")\n",
    "\n",
    "# Test model creation\n",
    "print(\"Testing model creation...\")\n",
    "model = create_autoencoder(\n",
    "    architecture_name='simple_linear',\n",
    "    input_size=32*32,  # For 32x32 images\n",
    "    latent_dim=16\n",
    ")\n",
    "print(f\"✅ Model created: {type(model).__name__}\")\n",
    "\n",
    "# Test a very short training run\n",
    "print(\"\\\\nTesting short training run...\")\n",
    "try:\n",
    "    trained_model, history = wrapper.experiment_runner.train_autoencoder(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        test_data=test_data,\n",
    "        test_labels=test_labels,\n",
    "        epochs=2,  # Very short for testing\n",
    "        learning_rate=0.001,\n",
    "        class_names=class_names,\n",
    "        save_model=False,  # Don't save for testing\n",
    "        experiment_name=\\\"test_experiment\\\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\n✅ Training completed successfully!\")\n",
    "    print(f\"   Final train loss: {history.get('final_train_loss', 'N/A')}\")\n",
    "    print(f\"   Final test loss: {history.get('final_test_loss', 'N/A')}\")\n",
    "    print(f\"   Training time: {history.get('training_time', 'N/A'):.2f}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1588c516",
   "metadata": {},
   "source": [
    "# Test Results Summary\n",
    "\n",
    "Run the cells above in order to test the wrapper functionality:\n",
    "\n",
    "1. **Cell 1**: Test imports - should show all modules loading correctly\n",
    "2. **Cell 2**: Define wrapper class - creates the main ExperimentRunnerWrapper  \n",
    "3. **Cell 3**: Initialize wrapper and test dataset - creates small test dataset\n",
    "4. **Cell 4**: Test model creation and training - runs a 2-epoch training test\n",
    "\n",
    "## Expected Behavior:\n",
    "- All imports should work without errors\n",
    "- Dataset generation should create 8 samples per class (16 total)\n",
    "- Model creation should succeed for 'simple_linear' architecture  \n",
    "- Training should complete in a few seconds with loss values\n",
    "\n",
    "## Common Issues to Watch For:\n",
    "- Import errors (we fixed these in the runner.py file)\n",
    "- Architecture name mismatches (use names from MODEL_ARCHITECTURES)\n",
    "- Memory issues (we use small datasets for testing)\n",
    "- Visualization backend issues (matplotlib setup)\n",
    "\n",
    "If all cells run successfully, the wrapper is working correctly! 🎉"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
