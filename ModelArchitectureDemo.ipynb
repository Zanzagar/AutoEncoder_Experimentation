{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "554ee85a",
   "metadata": {},
   "source": [
    "# AutoEncoder Model Architecture Demo\n",
    "\n",
    "This notebook demonstrates how to initialize and use different autoencoder architectures from the `autoencoder_lib` package for 64x64 grayscale images.\n",
    "\n",
    "## Available Architectures\n",
    "\n",
    "1. **SimpleLinearAutoencoder**: Basic fully-connected architecture\n",
    "2. **DeeperLinearAutoencoder**: Deeper fully-connected architecture  \n",
    "3. **ConvAutoencoder**: Basic convolutional architecture\n",
    "4. **DeeperConvAutoencoder**: Advanced convolutional architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c69d6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported all model architectures!\n",
      "Available architectures: ['simple_linear', 'deeper_linear', 'convolutional', 'deeper_convolutional']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from autoencoder_lib.models import (\n",
    "    SimpleLinearAutoencoder, \n",
    "    DeeperLinearAutoencoder,\n",
    "    ConvAutoencoder, \n",
    "    DeeperConvAutoencoder,\n",
    "    ModelFactory,\n",
    "    create_autoencoder\n",
    ")\n",
    "\n",
    "print(\"Successfully imported all model architectures!\")\n",
    "print(f\"Available architectures: {ModelFactory.get_available_architectures()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf7501",
   "metadata": {},
   "source": [
    "## Model Initialization Parameters\n",
    "\n",
    "For 64x64 grayscale images:\n",
    "- **Input size for linear models**: 64 × 64 = 4096 (flattened)\n",
    "- **Input channels for conv models**: 1 (grayscale)\n",
    "- **Latent dimension**: We'll use 16 for demonstration (adjustable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "840d6fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size (flattened): 4096\n",
      "Input channels: 1\n",
      "Latent dimension: 16\n",
      "Image shape: (1, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "# Parameters for 64x64 grayscale images\n",
    "INPUT_SIZE = 64 * 64  # Flattened size for linear models\n",
    "INPUT_CHANNELS = 1    # Grayscale images\n",
    "LATENT_DIM = 16       # Latent space dimension\n",
    "IMAGE_SHAPE = (1, 64, 64)  # (channels, height, width)\n",
    "\n",
    "print(f\"Input size (flattened): {INPUT_SIZE}\")\n",
    "print(f\"Input channels: {INPUT_CHANNELS}\")\n",
    "print(f\"Latent dimension: {LATENT_DIM}\")\n",
    "print(f\"Image shape: {IMAGE_SHAPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03037387",
   "metadata": {},
   "source": [
    "## 1. Linear Autoencoder Architectures\n",
    "\n",
    "Linear autoencoders flatten the input image and use fully-connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8202af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Linear autoencoders initialized successfully!\n",
      "Simple Linear info: {'model_class': 'SimpleLinearAutoencoder', 'latent_dim': 16, 'total_params': 1057040, 'trainable_params': 1057040, 'input_size': 4096, 'architecture_type': 'linear'}\n",
      "Deeper Linear info: {'model_class': 'DeeperLinearAutoencoder', 'latent_dim': 16, 'total_params': 2186128, 'trainable_params': 2186128, 'input_size': 4096, 'architecture_type': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Direct instantiation\n",
    "simple_linear = SimpleLinearAutoencoder(\n",
    "    input_size=INPUT_SIZE, \n",
    "    latent_dim=LATENT_DIM\n",
    ")\n",
    "\n",
    "deeper_linear = DeeperLinearAutoencoder(\n",
    "    input_size=INPUT_SIZE,\n",
    "    latent_dim=LATENT_DIM\n",
    ")\n",
    "\n",
    "# Method 2: Using factory\n",
    "simple_linear_factory = create_autoencoder(\n",
    "    'simple_linear',\n",
    "    input_size=INPUT_SIZE,\n",
    "    latent_dim=LATENT_DIM\n",
    ")\n",
    "\n",
    "deeper_linear_factory = ModelFactory.create_model(\n",
    "    'deeper_linear',\n",
    "    input_size=INPUT_SIZE,\n",
    "    latent_dim=LATENT_DIM\n",
    ")\n",
    "\n",
    "print(\"✅ Linear autoencoders initialized successfully!\")\n",
    "print(f\"Simple Linear info: {simple_linear.get_model_info()}\")\n",
    "print(f\"Deeper Linear info: {deeper_linear.get_model_info()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2a0ef3",
   "metadata": {},
   "source": [
    "## 2. Convolutional Autoencoder Architectures\n",
    "\n",
    "Convolutional autoencoders use CNN layers to preserve spatial relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c4c2958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Convolutional autoencoders initialized successfully!\n",
      "Conv info: {'model_class': 'ConvAutoencoder', 'latent_dim': 16, 'total_params': 181713, 'trainable_params': 181713, 'input_channels': 1, 'input_size': (64, 64), 'architecture_type': 'convolutional'}\n",
      "Deeper Conv info: {'model_class': 'DeeperConvAutoencoder', 'latent_dim': 16, 'total_params': 4992273, 'trainable_params': 4992273, 'input_channels': 1, 'input_size': (64, 64), 'architecture_type': 'convolutional'}\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Direct instantiation\n",
    "conv_autoencoder = ConvAutoencoder(\n",
    "    input_channels=INPUT_CHANNELS,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    input_size=(64, 64)\n",
    ")\n",
    "\n",
    "deeper_conv_autoencoder = DeeperConvAutoencoder(\n",
    "    input_channels=INPUT_CHANNELS,\n",
    "    latent_dim=LATENT_DIM,  # Note: DeeperConv typically uses larger latent_dim (default 32)\n",
    "    input_size=(64, 64)\n",
    ")\n",
    "\n",
    "# Method 2: Using factory\n",
    "conv_factory = create_autoencoder(\n",
    "    'convolutional',\n",
    "    input_channels=INPUT_CHANNELS,\n",
    "    latent_dim=LATENT_DIM\n",
    ")\n",
    "\n",
    "deeper_conv_factory = ModelFactory.create_model(\n",
    "    'deeper_convolutional',\n",
    "    input_channels=INPUT_CHANNELS,\n",
    "    latent_dim=LATENT_DIM\n",
    ")\n",
    "\n",
    "print(\"✅ Convolutional autoencoders initialized successfully!\")\n",
    "print(f\"Conv info: {conv_autoencoder.get_model_info()}\")\n",
    "print(f\"Deeper Conv info: {deeper_conv_autoencoder.get_model_info()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864eecc4",
   "metadata": {},
   "source": [
    "## 3. Model Comparison\n",
    "\n",
    "Let's compare the parameter counts and architectures of all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8590112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Model Architecture Comparison\n",
      "==================================================\n",
      "SimpleLinearAutoencoder   |  1,057,040 params |          linear architecture\n",
      "DeeperLinearAutoencoder   |  2,186,128 params |          linear architecture\n",
      "ConvAutoencoder           |    181,713 params |   convolutional architecture\n",
      "DeeperConvAutoencoder     |  4,992,273 params |   convolutional architecture\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'SimpleLinearAutoencoder': simple_linear,\n",
    "    'DeeperLinearAutoencoder': deeper_linear,\n",
    "    'ConvAutoencoder': conv_autoencoder,\n",
    "    'DeeperConvAutoencoder': deeper_conv_autoencoder\n",
    "}\n",
    "\n",
    "print(\"📊 Model Architecture Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, model in models.items():\n",
    "    info = model.get_model_info()\n",
    "    params = info['total_params']\n",
    "    arch_type = info.get('architecture_type', 'N/A')\n",
    "    \n",
    "    print(f\"{name:25} | {params:>10,} params | {arch_type:>15} architecture\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57974f13",
   "metadata": {},
   "source": [
    "## 4. Testing Forward Pass\n",
    "\n",
    "Let's test each model with dummy 64x64 grayscale images to ensure they work correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8dd1988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 1, 64, 64])\n",
      "\n",
      "🧪 Testing forward pass for each model:\n",
      "==================================================\n",
      "✅ SimpleLinearAutoencoder  \n",
      "   Input:   torch.Size([4, 1, 64, 64])\n",
      "   Encoded: torch.Size([4, 16])\n",
      "   Decoded: torch.Size([4, 1, 64, 64])\n",
      "   Latent dim: 16\n",
      "\n",
      "✅ DeeperLinearAutoencoder  \n",
      "   Input:   torch.Size([4, 1, 64, 64])\n",
      "   Encoded: torch.Size([4, 16])\n",
      "   Decoded: torch.Size([4, 1, 64, 64])\n",
      "   Latent dim: 16\n",
      "\n",
      "✅ ConvAutoencoder          \n",
      "   Input:   torch.Size([4, 1, 64, 64])\n",
      "   Encoded: torch.Size([4, 16])\n",
      "   Decoded: torch.Size([4, 1, 64, 64])\n",
      "   Latent dim: 16\n",
      "\n",
      "✅ DeeperConvAutoencoder    \n",
      "   Input:   torch.Size([4, 1, 64, 64])\n",
      "   Encoded: torch.Size([4, 16])\n",
      "   Decoded: torch.Size([4, 1, 64, 64])\n",
      "   Latent dim: 16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dummy input data (batch of 4 images)\n",
    "batch_size = 4\n",
    "dummy_input = torch.randn(batch_size, 1, 64, 64)  # (batch, channels, height, width)\n",
    "\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(\"\\n🧪 Testing forward pass for each model:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Forward pass: get encoded and decoded outputs\n",
    "            encoded, decoded = model(dummy_input)\n",
    "            \n",
    "        print(f\"✅ {name:25}\")\n",
    "        print(f\"   Input:   {dummy_input.shape}\")\n",
    "        print(f\"   Encoded: {encoded.shape}\")  \n",
    "        print(f\"   Decoded: {decoded.shape}\")\n",
    "        print(f\"   Latent dim: {encoded.shape[1]}\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {name:25} - Error: {e}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28042e8a",
   "metadata": {},
   "source": [
    "## 5. Using ModelFactory for Configuration-based Creation\n",
    "\n",
    "The ModelFactory supports creating models from configuration dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faedf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configurations for different models\n",
    "configs = [\n",
    "    {\n",
    "        'architecture': 'simple_linear',\n",
    "        'input_size': 64*64,\n",
    "        'latent_dim': 32\n",
    "    },\n",
    "    {\n",
    "        'architecture': 'convolutional', \n",
    "        'input_channels': 1,\n",
    "        'latent_dim': 64\n",
    "    },\n",
    "    {\n",
    "        'architecture': 'deeper_convolutional',\n",
    "        'input_channels': 1,\n",
    "        'latent_dim': 128,\n",
    "        'input_size': (64, 64)\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"🏭 Creating models from configuration dictionaries:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for i, config in enumerate(configs, 1):\n",
    "    try:\n",
    "        model = ModelFactory.create_model_from_config(config)\n",
    "        info = model.get_model_info()\n",
    "        \n",
    "        print(f\"Config {i}: {config['architecture']}\")\n",
    "        print(f\"   ✅ Created successfully\")\n",
    "        print(f\"   📊 Parameters: {info['total_params']:,}\")\n",
    "        print(f\"   🔧 Latent dim: {info['latent_dim']}\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Config {i}: ❌ Error: {e}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8667d6b4",
   "metadata": {},
   "source": [
    "## 6. Architecture Information Summary\n",
    "\n",
    "Get detailed information about each available architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0e6656",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📋 Detailed Architecture Information:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for arch_name in ModelFactory.get_available_architectures():\n",
    "    try:\n",
    "        info = ModelFactory.get_model_info(arch_name)\n",
    "        \n",
    "        print(f\"\\n🏗️  {arch_name.upper()}\")\n",
    "        print(f\"   Class: {info['class_name']}\")\n",
    "        print(f\"   Type: {info['architecture_type']}\")\n",
    "        print(f\"   Parameters: {info['total_params']:,}\")\n",
    "        print(f\"   Latent Dimension: {info['latent_dim']}\")\n",
    "        \n",
    "        # Print description if available\n",
    "        if info.get('docstring'):\n",
    "            desc = info['docstring'].split('\\n')[0].strip()\n",
    "            if desc:\n",
    "                print(f\"   Description: {desc}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error getting info for {arch_name}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎉 All model architectures are ready for experimentation!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
