{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afee59f9",
   "metadata": {},
   "source": [
    "# AutoEncoder Wrapper Demo - Comprehensive Testing\n",
    "\n",
    "This notebook demonstrates the complete autoencoder wrapper functionality, including:\n",
    "\n",
    "1. **Environment Setup** - Import testing and system verification\n",
    "2. **Dataset Generation** - Creating test datasets with proper formatting\n",
    "3. **Model Architecture Testing** - Verifying all model types work correctly  \n",
    "4. **Data Pipeline Debugging** - Ensuring data shapes and formats are correct\n",
    "5. **Training Pipeline** - Running complete experiments with proper error handling\n",
    "6. **Results Analysis** - Visualization and performance evaluation\n",
    "7. **Wrapper Integration** - Testing the full ExperimentRunner workflow\n",
    "\n",
    "## üéØ Goal: Create a robust, debugged workflow for autoencoder experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef15467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AUTOENCODER WRAPPER DEMO - COMPREHENSIVE TESTING\n",
      "============================================================\n",
      "\\n1. Testing Environment Setup...\n",
      "‚úÖ Core libraries imported successfully\n",
      "   PyTorch: 2.7.0+cpu\n",
      "   Device available: cpu\n",
      "‚ùå AutoEncoder library import failed: No module named 'autoencoder_lib.data.generators'\n",
      "   Make sure autoencoder_lib is properly installed\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 30\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mautoencoder_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_dataset, get_available_dataset_types\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mautoencoder_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfactory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_autoencoder, get_available_architectures\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'autoencoder_lib.data.generators'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[1], line 39\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Make sure autoencoder_lib is properly installed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m     \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Display available configurations\u001b[39;00m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\PycharmProjects\\AutoEncoder_Experimentation\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2121\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[0;32m   2119\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2120\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 2121\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInteractiveTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exception_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2122\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2125\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcontains_exceptiongroup\u001b[39m(val):\n",
      "File \u001b[1;32mc:\\PycharmProjects\\AutoEncoder_Experimentation\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[0;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \n\u001b[0;32m    705\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mListTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\PycharmProjects\\AutoEncoder_Experimentation\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    565\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    567\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 568\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchained_exc_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    572\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchained_exceptions_tb_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[1;32mc:\\PycharmProjects\\AutoEncoder_Experimentation\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py:1435\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[1;32m-> 1435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1436\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\PycharmProjects\\AutoEncoder_Experimentation\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py:1326\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1323\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32mc:\\PycharmProjects\\AutoEncoder_Experimentation\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py:1173\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1166\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1170\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   1171\u001b[0m ):\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1176\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[0;32m   1177\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[1;32mc:\\PycharmProjects\\AutoEncoder_Experimentation\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py:1063\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m   1061\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[0;32m   1062\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1063\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m   1064\u001b[0m )\n\u001b[0;32m   1066\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1067\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\PycharmProjects\\AutoEncoder_Experimentation\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py:1131\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1131\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(\u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtb_frame\u001b[49m)\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "# 1. ENVIRONMENT SETUP AND IMPORT TESTING\n",
    "print(\"=\" * 60)\n",
    "print(\"AUTOENCODER WRAPPER DEMO - COMPREHENSIVE TESTING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\\\n1. Testing Environment Setup...\")\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Test core imports\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    print(\"‚úÖ Core libraries imported successfully\")\n",
    "    print(f\"   PyTorch: {torch.__version__}\")\n",
    "    print(f\"   Device available: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Core library import failed: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Test autoencoder_lib imports\n",
    "try:\n",
    "    from autoencoder_lib.data.generators import generate_dataset, get_available_dataset_types\n",
    "    from autoencoder_lib.models.factory import create_autoencoder, get_available_architectures\n",
    "    from autoencoder_lib.experiment.runner import ExperimentRunner\n",
    "    from autoencoder_lib.visualization.latent_viz import visualize_latent_space_2d\n",
    "    from autoencoder_lib.visualization.reconstruction_viz import visualize_reconstructions\n",
    "    print(\"‚úÖ AutoEncoder library imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå AutoEncoder library import failed: {e}\")\n",
    "    print(\"   Make sure autoencoder_lib is properly installed\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Display available configurations\n",
    "print(\"\\\\nüìä Available Configurations:\")\n",
    "try:\n",
    "    architectures = get_available_architectures()\n",
    "    print(f\"   Architectures: {architectures}\")\n",
    "except:\n",
    "    print(\"   Architectures: Could not retrieve\")\n",
    "\n",
    "try:\n",
    "    dataset_types = get_available_dataset_types()\n",
    "    print(f\"   Dataset types: {dataset_types}\")\n",
    "except:\n",
    "    print(\"   Dataset types: Could not retrieve\")\n",
    "\n",
    "print(\"\\\\n‚úÖ Environment setup complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c1eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DATASET GENERATION WITH PROPER ERROR HANDLING\n",
    "print(\"\\\\n2. Dataset Generation and Validation...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Clean up any existing test datasets\n",
    "test_dataset_dir = \"comprehensive_wrapper_test\"\n",
    "if os.path.exists(test_dataset_dir):\n",
    "    import shutil\n",
    "    shutil.rmtree(test_dataset_dir)\n",
    "    print(f\"   Cleaned up existing {test_dataset_dir}\")\n",
    "\n",
    "# Generate a small but comprehensive test dataset\n",
    "try:\n",
    "    dataset_info = generate_dataset(\n",
    "        dataset_type='layered_geological',\n",
    "        output_dir=test_dataset_dir,\n",
    "        num_samples_per_class=8,  # Small but sufficient for testing\n",
    "        image_size=(32, 32),\n",
    "        num_classes=3,  # Keep it simple: 3 classes\n",
    "        random_seed=42,\n",
    "        save_examples=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Dataset generated successfully!\")\n",
    "    print(f\"   Directory: {test_dataset_dir}\")\n",
    "    print(f\"   Classes: {dataset_info['label_names']}\")\n",
    "    print(f\"   Image size: {dataset_info['image_size']}\")\n",
    "    print(f\"   Total samples: {dataset_info['total_samples']}\")\n",
    "    print(f\"   Samples per class: {dataset_info['samples_per_class']}\")\n",
    "    \n",
    "    # Verify files were created\n",
    "    total_files = 0\n",
    "    for class_name in dataset_info['label_names']:\n",
    "        class_dir = Path(test_dataset_dir) / class_name\n",
    "        if class_dir.exists():\n",
    "            files = list(class_dir.glob(\"*.png\"))\n",
    "            print(f\"   {class_name}: {len(files)} files\")\n",
    "            total_files += len(files)\n",
    "        else:\n",
    "            print(f\"   ‚ùå {class_name}: directory not found\")\n",
    "    \n",
    "    print(f\"   Total files created: {total_files}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Dataset generation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(\"\\\\n‚úÖ Dataset generation complete!\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77add53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. MODEL ARCHITECTURE TESTING AND VALIDATION\n",
    "print(\"\\\\n3. Model Architecture Testing...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Test different model architectures with proper parameters\n",
    "image_size = dataset_info['image_size']\n",
    "input_size = image_size[0] * image_size[1]  # 32 * 32 = 1024\n",
    "input_channels = 1  # Grayscale\n",
    "latent_dim = 16  # Reasonable latent dimension\n",
    "\n",
    "models_to_test = [\n",
    "    ('simple_linear', {'input_size': input_size, 'latent_dim': latent_dim}),\n",
    "    ('deeper_linear', {'input_size': input_size, 'latent_dim': latent_dim}),\n",
    "    ('convolutional', {'input_channels': input_channels, 'latent_dim': latent_dim}),\n",
    "    ('deeper_convolutional', {'input_channels': input_channels, 'latent_dim': latent_dim})\n",
    "]\n",
    "\n",
    "created_models = {}\n",
    "\n",
    "for arch_name, params in models_to_test:\n",
    "    try:\n",
    "        print(f\"\\\\n   Testing {arch_name}...\")\n",
    "        print(f\"   Parameters: {params}\")\n",
    "        \n",
    "        model = create_autoencoder(architecture_name=arch_name, **params)\n",
    "        \n",
    "        # Test model with dummy input\n",
    "        if 'input_size' in params:\n",
    "            # Linear models expect flattened input\n",
    "            dummy_input = torch.randn(1, input_size)\n",
    "            expected_shape = (1, input_size)\n",
    "        else:\n",
    "            # Convolutional models expect image input\n",
    "            dummy_input = torch.randn(1, input_channels, *image_size)\n",
    "            expected_shape = (1, input_channels, *image_size)\n",
    "        \n",
    "        print(f\"   Input shape: {dummy_input.shape}\")\n",
    "        \n",
    "        # Test forward pass\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            encoded, decoded = model(dummy_input)\n",
    "        \n",
    "        print(f\"   ‚úÖ {arch_name} created successfully!\")\n",
    "        print(f\"      Encoded shape: {encoded.shape}\")\n",
    "        print(f\"      Decoded shape: {decoded.shape}\")\n",
    "        print(f\"      Expected shape: {expected_shape}\")\n",
    "        \n",
    "        # Verify output shape matches input\n",
    "        if decoded.shape == dummy_input.shape:\n",
    "            print(f\"      ‚úÖ Output shape matches input!\")\n",
    "            created_models[arch_name] = {'model': model, 'params': params}\n",
    "        else:\n",
    "            print(f\"      ‚ùå Shape mismatch! Expected {dummy_input.shape}, got {decoded.shape}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå {arch_name} failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\\\n‚úÖ Model testing complete! Successfully created {len(created_models)} models\")\n",
    "print(f\"   Working architectures: {list(created_models.keys())}\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb7d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. DATA LOADING AND PREPROCESSING PIPELINE\n",
    "print(\"\\\\n4. Data Loading and Preprocessing...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def load_dataset_with_proper_shapes(dataset_dir, class_names, image_size):\n",
    "    \"\"\"\n",
    "    Load dataset ensuring proper shapes for both linear and convolutional models.\n",
    "    Returns data in both formats.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(f\"   Loading from: {dataset_dir}\")\n",
    "    print(f\"   Expected classes: {class_names}\")\n",
    "    \n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_dir = Path(dataset_dir) / class_name\n",
    "        print(f\"   Processing class {class_idx}: {class_name}\")\n",
    "        \n",
    "        if not class_dir.exists():\n",
    "            print(f\"      ‚ùå Directory not found: {class_dir}\")\n",
    "            continue\n",
    "            \n",
    "        image_files = list(class_dir.glob(\"*.png\"))\n",
    "        print(f\"      Found {len(image_files)} images\")\n",
    "        \n",
    "        for img_file in image_files:\n",
    "            try:\n",
    "                # Load and preprocess image\n",
    "                img = Image.open(img_file).convert('L')  # Grayscale\n",
    "                \n",
    "                # Resize if necessary\n",
    "                if img.size != image_size:\n",
    "                    img = img.resize(image_size, Image.Resampling.LANCZOS)\n",
    "                \n",
    "                # Convert to numpy array and normalize\n",
    "                img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "                \n",
    "                all_data.append(img_array)\n",
    "                all_labels.append(class_idx)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"      ‚ùå Failed to load {img_file}: {e}\")\n",
    "    \n",
    "    if len(all_data) == 0:\n",
    "        raise ValueError(\"No data loaded! Check dataset directory and file formats.\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    data_array = np.array(all_data)  # Shape: (N, H, W)\n",
    "    labels_array = np.array(all_labels)\n",
    "    \n",
    "    print(f\"   Loaded {len(data_array)} samples\")\n",
    "    print(f\"   Data shape: {data_array.shape}\")\n",
    "    print(f\"   Label distribution: {np.bincount(labels_array)}\")\n",
    "    \n",
    "    # Create tensors in different formats\n",
    "    # Convolutional format: (N, C, H, W)\n",
    "    conv_data = torch.tensor(data_array, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
    "    \n",
    "    # Linear format: (N, H*W) - flattened\n",
    "    linear_data = torch.tensor(data_array, dtype=torch.float32).view(len(data_array), -1)\n",
    "    \n",
    "    # Labels\n",
    "    labels_tensor = torch.tensor(labels_array, dtype=torch.long)\n",
    "    \n",
    "    return conv_data, linear_data, labels_tensor\n",
    "\n",
    "try:\n",
    "    # Load the dataset\n",
    "    class_names = dataset_info['label_names']\n",
    "    image_size = dataset_info['image_size']\n",
    "    \n",
    "    conv_data, linear_data, labels = load_dataset_with_proper_shapes(\n",
    "        test_dataset_dir, class_names, image_size\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ Data loaded successfully!\")\n",
    "    print(f\"   Convolutional format: {conv_data.shape}\")\n",
    "    print(f\"   Linear format: {linear_data.shape}\")\n",
    "    print(f\"   Labels: {labels.shape}\")\n",
    "    print(f\"   Value range: [{conv_data.min():.3f}, {conv_data.max():.3f}]\")\n",
    "    \n",
    "    # Split into train/test\n",
    "    split_idx = int(len(conv_data) * 0.7)\n",
    "    \n",
    "    conv_train, conv_test = conv_data[:split_idx], conv_data[split_idx:]\n",
    "    linear_train, linear_test = linear_data[:split_idx], linear_data[split_idx:]\n",
    "    labels_train, labels_test = labels[:split_idx], labels[split_idx:]\n",
    "    \n",
    "    print(f\"\\\\nüìä Data splits:\")\n",
    "    print(f\"   Train samples: {len(conv_train)}\")\n",
    "    print(f\"   Test samples: {len(conv_test)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Data loading failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(\"\\\\n‚úÖ Data preprocessing complete!\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151372f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. DATALOADER CREATION AND VALIDATION\n",
    "print(\"\\\\n5. DataLoader Creation and Validation...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def create_dataloaders(conv_train, linear_train, labels_train, batch_size=4):\n",
    "    \"\"\"Create DataLoaders for both convolutional and linear models.\"\"\"\n",
    "    \n",
    "    dataloaders = {}\n",
    "    \n",
    "    # Convolutional DataLoader\n",
    "    # For autoencoders: (input, target, labels) where input=target for reconstruction\n",
    "    conv_dataset = TensorDataset(conv_train, conv_train, labels_train)\n",
    "    conv_loader = DataLoader(conv_dataset, batch_size=batch_size, shuffle=True)\n",
    "    dataloaders['convolutional'] = conv_loader\n",
    "    \n",
    "    # Linear DataLoader\n",
    "    linear_dataset = TensorDataset(linear_train, linear_train, labels_train)\n",
    "    linear_loader = DataLoader(linear_dataset, batch_size=batch_size, shuffle=True)\n",
    "    dataloaders['linear'] = linear_loader\n",
    "    \n",
    "    return dataloaders\n",
    "\n",
    "try:\n",
    "    # Create DataLoaders\n",
    "    batch_size = 4  # Small batch size for testing\n",
    "    dataloaders = create_dataloaders(conv_train, linear_train, labels_train, batch_size)\n",
    "    \n",
    "    print(f\"‚úÖ DataLoaders created successfully!\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    \n",
    "    # Test DataLoaders\n",
    "    for model_type, loader in dataloaders.items():\n",
    "        print(f\"\\\\n   Testing {model_type} DataLoader:\")\n",
    "        print(f\"      Total batches: {len(loader)}\")\n",
    "        \n",
    "        # Get first batch and inspect\n",
    "        first_batch = next(iter(loader))\n",
    "        x, y, labels_batch = first_batch\n",
    "        \n",
    "        print(f\"      Batch input shape: {x.shape}\")\n",
    "        print(f\"      Batch target shape: {y.shape}\")\n",
    "        print(f\"      Batch labels shape: {labels_batch.shape}\")\n",
    "        print(f\"      Input/target equal: {torch.equal(x, y)}\")\n",
    "        \n",
    "        # Verify shapes match expected model inputs\n",
    "        if model_type == 'convolutional':\n",
    "            expected_shape = (batch_size, 1, *image_size)\n",
    "        else:\n",
    "            expected_shape = (batch_size, input_size)\n",
    "        \n",
    "        if x.shape == expected_shape or x.shape[0] <= batch_size:  # Last batch might be smaller\n",
    "            print(f\"      ‚úÖ Shape matches expected: {expected_shape}\")\n",
    "        else:\n",
    "            print(f\"      ‚ùå Shape mismatch! Expected: {expected_shape}, Got: {x.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå DataLoader creation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(\"\\\\n‚úÖ DataLoader creation complete!\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36442906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. EXPERIMENT RUNNER TESTING WITH PROPER DEBUG\n",
    "print(\"\\\\n6. ExperimentRunner Testing...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def test_experiment_runner(architecture_name, model_params, train_loader, test_data, test_labels):\n",
    "    \"\"\"Test ExperimentRunner with a specific architecture.\"\"\"\n",
    "    \n",
    "    print(f\"\\\\nüß™ Testing {architecture_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Create ExperimentRunner\n",
    "        runner = ExperimentRunner(\n",
    "            output_dir=f\"test_output_{architecture_name}\",\n",
    "            random_seed=42\n",
    "        )\n",
    "        print(f\"   ‚úÖ ExperimentRunner created\")\n",
    "        \n",
    "        # Create model\n",
    "        model = create_autoencoder(architecture_name=architecture_name, **model_params)\n",
    "        print(f\"   ‚úÖ Model created: {model.__class__.__name__}\")\n",
    "        \n",
    "        # Test data compatibility\n",
    "        sample_batch = next(iter(train_loader))\n",
    "        sample_input = sample_batch[0][:1]  # First sample from batch\n",
    "        \n",
    "        print(f\"   Sample input shape: {sample_input.shape}\")\n",
    "        \n",
    "        # Test model forward pass\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            encoded, decoded = model(sample_input)\n",
    "            print(f\"   Model test - Encoded: {encoded.shape}, Decoded: {decoded.shape}\")\n",
    "        \n",
    "        # Run training (just 1 epoch for testing)\n",
    "        print(f\"   üöÄ Starting training...\")\n",
    "        \n",
    "        trained_model, history = runner.train_autoencoder(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            test_data=test_data,\n",
    "            test_labels=test_labels,\n",
    "            epochs=2,  # Very short for testing\n",
    "            learning_rate=0.001,\n",
    "            class_names=class_names,\n",
    "            save_model=False,  # Don't save during testing\n",
    "            experiment_name=f\"test_{architecture_name}\",\n",
    "            visualization_interval=999999,  # Disable intermediate visualizations\n",
    "            num_visualizations=1  # Only final visualization\n",
    "        )\n",
    "        \n",
    "        print(f\"   ‚úÖ Training completed successfully!\")\n",
    "        print(f\"      Final train loss: {history.get('final_train_loss', 'N/A')}\")\n",
    "        print(f\"      Final test loss: {history.get('final_test_loss', 'N/A')}\")\n",
    "        print(f\"      Training time: {history.get('training_time', 0):.2f}s\")\n",
    "        \n",
    "        return True, trained_model, history\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False, None, None\n",
    "\n",
    "# Test each working architecture\n",
    "successful_runs = {}\n",
    "\n",
    "for arch_name in ['simple_linear', 'convolutional']:  # Test key architectures first\n",
    "    if arch_name not in created_models:\n",
    "        print(f\"\\\\n‚è≠Ô∏è  Skipping {arch_name} - not available\")\n",
    "        continue\n",
    "    \n",
    "    model_params = created_models[arch_name]['params']\n",
    "    \n",
    "    # Select appropriate dataloader and test data\n",
    "    if 'input_size' in model_params:\n",
    "        # Linear model\n",
    "        train_loader = dataloaders['linear']\n",
    "        test_data = linear_test\n",
    "    else:\n",
    "        # Convolutional model  \n",
    "        train_loader = dataloaders['convolutional']\n",
    "        test_data = conv_test\n",
    "    \n",
    "    success, trained_model, history = test_experiment_runner(\n",
    "        arch_name, model_params, train_loader, test_data, labels_test\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        successful_runs[arch_name] = {\n",
    "            'model': trained_model,\n",
    "            'history': history,\n",
    "            'params': model_params\n",
    "        }\n",
    "\n",
    "print(f\"\\\\nüéâ Experiment Runner Testing Complete!\")\n",
    "print(f\"   Successful runs: {len(successful_runs)}\")\n",
    "print(f\"   Working architectures: {list(successful_runs.keys())}\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce86877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. COMPREHENSIVE WRAPPER CLASS IMPLEMENTATION\n",
    "print(\"\\\\n7. Comprehensive Wrapper Implementation...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "class AutoEncoderExperimentWrapper:\n",
    "    \"\"\"\n",
    "    Comprehensive wrapper for autoencoder experimentation.\n",
    "    Handles all the complexity of data loading, model creation, and training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir=\"wrapper_experiments\", random_seed=42):\n",
    "        self.output_dir = output_dir\n",
    "        self.random_seed = random_seed\n",
    "        self.runner = ExperimentRunner(output_dir=output_dir, random_seed=random_seed)\n",
    "        \n",
    "        # Storage for experiment results\n",
    "        self.experiment_results = {}\n",
    "        self.dataset_info = None\n",
    "        self.data_cache = {}\n",
    "        \n",
    "        print(f\"‚úÖ AutoEncoderExperimentWrapper initialized\")\n",
    "        print(f\"   Output directory: {output_dir}\")\n",
    "        print(f\"   Random seed: {random_seed}\")\n",
    "    \n",
    "    def prepare_dataset(self, dataset_type='layered_geological', output_dir=None, \n",
    "                       num_samples_per_class=10, image_size=(32, 32), num_classes=3):\n",
    "        \"\"\"Generate and load dataset for experiments.\"\"\"\n",
    "        \n",
    "        if output_dir is None:\n",
    "            output_dir = f\"wrapper_dataset_{dataset_type}\"\n",
    "        \n",
    "        print(f\"\\\\nüìä Preparing dataset...\")\n",
    "        print(f\"   Type: {dataset_type}\")\n",
    "        print(f\"   Output: {output_dir}\")\n",
    "        print(f\"   Samples per class: {num_samples_per_class}\")\n",
    "        print(f\"   Image size: {image_size}\")\n",
    "        print(f\"   Classes: {num_classes}\")\n",
    "        \n",
    "        # Generate dataset\n",
    "        self.dataset_info = generate_dataset(\n",
    "            dataset_type=dataset_type,\n",
    "            output_dir=output_dir,\n",
    "            num_samples_per_class=num_samples_per_class,\n",
    "            image_size=image_size,\n",
    "            num_classes=num_classes,\n",
    "            random_seed=self.random_seed,\n",
    "            save_examples=True\n",
    "        )\n",
    "        \n",
    "        # Load data in multiple formats - FIXED: Use correct variable names\n",
    "        conv_data, linear_data, labels = load_dataset_with_proper_shapes(\n",
    "            output_dir, self.dataset_info['label_names'], self.dataset_info['image_size']\n",
    "        )\n",
    "        \n",
    "        # Split train/test\n",
    "        split_idx = int(len(conv_data) * 0.7)\n",
    "        \n",
    "        self.data_cache = {\n",
    "            'conv_train': conv_data[:split_idx],\n",
    "            'conv_test': conv_data[split_idx:],\n",
    "            'linear_train': linear_data[:split_idx],\n",
    "            'linear_test': linear_data[split_idx:],\n",
    "            'labels_train': labels[:split_idx],\n",
    "            'labels_test': labels[split_idx:],\n",
    "            'class_names': self.dataset_info['label_names'],\n",
    "            'image_size': self.dataset_info['image_size']\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Dataset prepared successfully!\")\n",
    "        print(f\"   Total samples: {len(conv_data)}\")\n",
    "        print(f\"   Train: {len(self.data_cache['conv_train'])}\")\n",
    "        print(f\"   Test: {len(self.data_cache['conv_test'])}\")\n",
    "        \n",
    "        return self.dataset_info\n",
    "    \n",
    "    def run_single_experiment(self, architecture_name, latent_dim=16, epochs=5, \n",
    "                            learning_rate=0.001, batch_size=4):\n",
    "        \"\"\"Run a single autoencoder experiment.\"\"\"\n",
    "        \n",
    "        if not self.data_cache:\n",
    "            raise ValueError(\"Dataset not prepared. Call prepare_dataset() first.\")\n",
    "        \n",
    "        print(f\"\\\\nüß™ Running experiment: {architecture_name}\")\n",
    "        print(f\"   Latent dim: {latent_dim}\")\n",
    "        print(f\"   Epochs: {epochs}\")\n",
    "        print(f\"   Learning rate: {learning_rate}\")\n",
    "        print(f\"   Batch size: {batch_size}\")\n",
    "        \n",
    "        # Determine model parameters\n",
    "        image_size = self.data_cache['image_size']\n",
    "        \n",
    "        if architecture_name in ['simple_linear', 'deeper_linear']:\n",
    "            model_params = {\n",
    "                'input_size': image_size[0] * image_size[1],\n",
    "                'latent_dim': latent_dim\n",
    "            }\n",
    "            train_data = self.data_cache['linear_train']\n",
    "            test_data = self.data_cache['linear_test']\n",
    "        else:\n",
    "            model_params = {\n",
    "                'input_channels': 1,\n",
    "                'latent_dim': latent_dim,\n",
    "                'input_size': image_size  # Add input_size for convolutional models too\n",
    "            }\n",
    "            train_data = self.data_cache['conv_train']\n",
    "            test_data = self.data_cache['conv_test']\n",
    "        \n",
    "        # Create model\n",
    "        model = create_autoencoder(architecture_name=architecture_name, **model_params)\n",
    "        \n",
    "        # Create DataLoader\n",
    "        train_dataset = TensorDataset(train_data, train_data, self.data_cache['labels_train'])\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Run training\n",
    "        experiment_name = f\"{architecture_name}_latent{latent_dim}_lr{learning_rate}\"\n",
    "        \n",
    "        trained_model, history = self.runner.train_autoencoder(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            test_data=test_data,\n",
    "            test_labels=self.data_cache['labels_test'],\n",
    "            epochs=epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            class_names=self.data_cache['class_names'],\n",
    "            save_model=True,\n",
    "            experiment_name=experiment_name\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        self.experiment_results[experiment_name] = {\n",
    "            'architecture': architecture_name,\n",
    "            'model': trained_model,\n",
    "            'history': history,\n",
    "            'parameters': {\n",
    "                'latent_dim': latent_dim,\n",
    "                'epochs': epochs,\n",
    "                'learning_rate': learning_rate,\n",
    "                'batch_size': batch_size\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Experiment completed: {experiment_name}\")\n",
    "        print(f\"   Final test loss: {history.get('final_test_loss', 'N/A'):.4f}\")\n",
    "        \n",
    "        return experiment_name, history\n",
    "    \n",
    "    def run_systematic_experiments(self, architectures=None, latent_dims=None, epochs=5):\n",
    "        \"\"\"Run systematic experiments across multiple configurations.\"\"\"\n",
    "        \n",
    "        if architectures is None:\n",
    "            architectures = ['simple_linear', 'convolutional']\n",
    "        \n",
    "        if latent_dims is None:\n",
    "            latent_dims = [8, 16]\n",
    "        \n",
    "        print(f\"\\\\nüöÄ Running systematic experiments...\")\n",
    "        print(f\"   Architectures: {architectures}\")\n",
    "        print(f\"   Latent dims: {latent_dims}\")\n",
    "        print(f\"   Epochs: {epochs}\")\n",
    "        \n",
    "        results_summary = []\n",
    "        \n",
    "        for arch in architectures:\n",
    "            for latent_dim in latent_dims:\n",
    "                try:\n",
    "                    exp_name, history = self.run_single_experiment(\n",
    "                        architecture_name=arch,\n",
    "                        latent_dim=latent_dim,\n",
    "                        epochs=epochs\n",
    "                    )\n",
    "                    \n",
    "                    results_summary.append({\n",
    "                        'experiment': exp_name,\n",
    "                        'architecture': arch,\n",
    "                        'latent_dim': latent_dim,\n",
    "                        'final_test_loss': history.get('final_test_loss', float('inf')),\n",
    "                        'training_time': history.get('training_time', 0),\n",
    "                        'status': 'success'\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ùå Failed {arch} with latent_dim {latent_dim}: {e}\")\n",
    "                    results_summary.append({\n",
    "                        'experiment': f\"{arch}_latent{latent_dim}_failed\",\n",
    "                        'architecture': arch,\n",
    "                        'latent_dim': latent_dim,\n",
    "                        'final_test_loss': float('inf'),\n",
    "                        'training_time': 0,\n",
    "                        'status': f'failed: {str(e)[:50]}'\n",
    "                    })\n",
    "        \n",
    "        print(f\"\\\\nüìä Systematic experiments completed!\")\n",
    "        print(f\"   Total experiments: {len(results_summary)}\")\n",
    "        print(f\"   Successful: {sum(1 for r in results_summary if r['status'] == 'success')}\")\n",
    "        \n",
    "        return results_summary\n",
    "    \n",
    "    def get_experiment_summary(self):\n",
    "        \"\"\"Get summary of all experiments.\"\"\"\n",
    "        \n",
    "        if not self.experiment_results:\n",
    "            print(\"No experiments have been run yet.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\\\nüìã Experiment Summary ({len(self.experiment_results)} experiments):\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for exp_name, exp_data in self.experiment_results.items():\n",
    "            history = exp_data['history']\n",
    "            params = exp_data['parameters']\n",
    "            \n",
    "            print(f\"\\\\nüß™ {exp_name}\")\n",
    "            print(f\"   Architecture: {exp_data['architecture']}\")\n",
    "            print(f\"   Parameters: Latent={params['latent_dim']}, LR={params['learning_rate']}, Epochs={params['epochs']}\")\n",
    "            print(f\"   Final test loss: {history.get('final_test_loss', 'N/A'):.4f}\")\n",
    "            print(f\"   Training time: {history.get('training_time', 0):.2f}s\")\n",
    "            \n",
    "        return self.experiment_results\n",
    "\n",
    "# Test the wrapper\n",
    "print(f\"\\\\nüéØ Testing Comprehensive Wrapper...\")\n",
    "\n",
    "wrapper = AutoEncoderExperimentWrapper(\n",
    "    output_dir=\"comprehensive_wrapper_demo\",\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(\"\\\\n‚úÖ Wrapper created successfully!\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53257741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. WRAPPER DEMONSTRATION - SINGLE EXPERIMENT\n",
    "print(\"\\\\n8. Wrapper Demonstration - Single Experiment...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare dataset using wrapper\n",
    "print(\"\\\\nüìä Step 1: Preparing dataset...\")\n",
    "dataset_info = wrapper.prepare_dataset(\n",
    "    dataset_type='layered_geological',\n",
    "    num_samples_per_class=6,  # Small for quick demo\n",
    "    image_size=(32, 32),\n",
    "    num_classes=3\n",
    ")\n",
    "\n",
    "print(\"\\\\nüß™ Step 2: Running single experiment...\")\n",
    "try:\n",
    "    experiment_name, history = wrapper.run_single_experiment(\n",
    "        architecture_name='simple_linear',\n",
    "        latent_dim=8,\n",
    "        epochs=3,  # Quick demo\n",
    "        learning_rate=0.001,\n",
    "        batch_size=3\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ Single experiment completed successfully!\")\n",
    "    print(f\"   Experiment: {experiment_name}\")\n",
    "    print(f\"   Final loss: {history['final_test_loss']:.4f}\")\n",
    "    print(f\"   Time: {history['training_time']:.2f}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Single experiment failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be1cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. WRAPPER DEMONSTRATION - SYSTEMATIC EXPERIMENTS\n",
    "print(\"\\\\n9. Wrapper Demonstration - Systematic Experiments...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\\\nüöÄ Running systematic experiments...\")\n",
    "print(\"   This will test multiple architectures and latent dimensions\")\n",
    "\n",
    "try:\n",
    "    results_summary = wrapper.run_systematic_experiments(\n",
    "        architectures=['simple_linear', 'convolutional'],\n",
    "        latent_dims=[4, 8],  # Small dimensions for quick demo\n",
    "        epochs=2  # Very short for demo\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\nüìä Systematic Experiments Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for result in results_summary:\n",
    "        status_emoji = \"‚úÖ\" if result['status'] == 'success' else \"‚ùå\"\n",
    "        print(f\"{status_emoji} {result['experiment']}\")\n",
    "        print(f\"      Architecture: {result['architecture']}\")\n",
    "        print(f\"      Latent dim: {result['latent_dim']}\")\n",
    "        print(f\"      Test loss: {result['final_test_loss']:.4f}\")\n",
    "        print(f\"      Time: {result['training_time']:.2f}s\")\n",
    "        print(f\"      Status: {result['status']}\")\n",
    "        print()\n",
    "    \n",
    "    # Find best result\n",
    "    successful_results = [r for r in results_summary if r['status'] == 'success']\n",
    "    if successful_results:\n",
    "        best_result = min(successful_results, key=lambda x: x['final_test_loss'])\n",
    "        print(f\"üèÜ Best result: {best_result['experiment']}\")\n",
    "        print(f\"   Loss: {best_result['final_test_loss']:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Systematic experiments failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dcc651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. FINAL SUMMARY AND VALIDATION\n",
    "print(\"\\\\n10. Final Summary and Validation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get comprehensive summary\n",
    "print(\"\\\\nüìã Getting experiment summary...\")\n",
    "experiment_summary = wrapper.get_experiment_summary()\n",
    "\n",
    "print(\"\\\\nüîç Validation Checks:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Check 1: Dataset preparation\n",
    "if wrapper.dataset_info:\n",
    "    print(\"‚úÖ Dataset preparation: SUCCESS\")\n",
    "    print(f\"   Classes: {wrapper.dataset_info['label_names']}\")\n",
    "    print(f\"   Total samples: {wrapper.dataset_info['total_samples']}\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset preparation: FAILED\")\n",
    "\n",
    "# Check 2: Data caching\n",
    "if wrapper.data_cache:\n",
    "    print(\"‚úÖ Data caching: SUCCESS\")\n",
    "    print(f\"   Train samples: {len(wrapper.data_cache['conv_train'])}\")\n",
    "    print(f\"   Test samples: {len(wrapper.data_cache['conv_test'])}\")\n",
    "else:\n",
    "    print(\"‚ùå Data caching: FAILED\")\n",
    "\n",
    "# Check 3: Experiment execution\n",
    "if wrapper.experiment_results:\n",
    "    print(\"‚úÖ Experiment execution: SUCCESS\")\n",
    "    print(f\"   Completed experiments: {len(wrapper.experiment_results)}\")\n",
    "    \n",
    "    # Show best performing experiment\n",
    "    best_exp = None\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for exp_name, exp_data in wrapper.experiment_results.items():\n",
    "        test_loss = exp_data['history'].get('final_test_loss', float('inf'))\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            best_exp = exp_name\n",
    "    \n",
    "    if best_exp:\n",
    "        print(f\"   Best experiment: {best_exp}\")\n",
    "        print(f\"   Best test loss: {best_loss:.4f}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Experiment execution: FAILED\")\n",
    "\n",
    "# Check 4: File outputs\n",
    "output_dirs = [\n",
    "    wrapper.output_dir,\n",
    "    \"comprehensive_wrapper_test\",\n",
    "    \"comprehensive_wrapper_demo\"\n",
    "]\n",
    "\n",
    "files_created = 0\n",
    "for output_dir in output_dirs:\n",
    "    if os.path.exists(output_dir):\n",
    "        files = list(Path(output_dir).rglob(\"*\"))\n",
    "        files_created += len(files)\n",
    "\n",
    "if files_created > 0:\n",
    "    print(f\"‚úÖ File outputs: SUCCESS ({files_created} files created)\")\n",
    "else:\n",
    "    print(\"‚ùå File outputs: NO FILES CREATED\")\n",
    "\n",
    "print(f\"\\\\nüéâ COMPREHENSIVE WRAPPER DEMO COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\\\nüìù Summary:\")\n",
    "print(f\"   ‚úÖ Environment setup and imports working\")\n",
    "print(f\"   ‚úÖ Dataset generation and loading working\")\n",
    "print(f\"   ‚úÖ Model architectures tested and validated\")\n",
    "print(f\"   ‚úÖ Data preprocessing pipeline working\")\n",
    "print(f\"   ‚úÖ ExperimentRunner integration working\")\n",
    "print(f\"   ‚úÖ Comprehensive wrapper class implemented\")\n",
    "print(f\"   ‚úÖ Single and systematic experiments working\")\n",
    "print(\"\\\\nüöÄ The autoencoder wrapper is ready for full-scale experimentation!\")\n",
    "print(\"\\\\nüí° Next steps:\")\n",
    "print(\"   1. Use wrapper.prepare_dataset() to generate larger datasets\")\n",
    "print(\"   2. Use wrapper.run_systematic_experiments() for comprehensive analysis\")\n",
    "print(\"   3. Extend wrapper class for custom experiment configurations\")\n",
    "print(\"   4. Integrate with visualization and analysis tools\")\n",
    "print(\"\\\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
